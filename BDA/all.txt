########## All Aim ##########

prac 1
Installing and setting environment variables for Working with Apache Hadoop.

prac 2
Implementing Map-Reduce Program for Word Count problem

prac 3
Install Hive and use Hive Create and store structured databases.

prac 4
Download and install Spark.

prac 5
Install HBase and use the HBase Data model Store and retrieve data.

prac 6
Perform importing and exporting of data between SQL and Hadoop using Sqoop

prac 7
Write a Pig Script for solving counting problems.

prac 8
Install and Configure Flume. Use Flume and transport the data from the various sources to a centralized data store.



########## P1 ##########


###Open powershell with run as administrator

dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart

dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart

wsl –install

#### Restart ur PC

#download this file and install
https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi

### Restart ur PC after Pc starts WSL will directly get open
### enter username and password as Patkar
### start firing commands

sudo apt-get update

sudo apt-get install openjdk-8-jdk

java -version

sudo addgroup hadoop

sudo adduser --ingroup hadoop universal_user

sudo usermod -aG sudo universal_user

su universal_user

ssh-keygen -t rsa -P ""

cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

sudo nano /etc/sysctl.conf
###now paste this code in ubuntu shell -> 

net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

###press Ctrl+X, Shift Y and Enter

cd /usr/local

sudo wget https://mirrors.estointernet.in/apache/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz

sudo tar xzf hadoop-3.3.1.tar.gz

sudo mv hadoop-3.3.1 hadoop

sudo chown -R universal_user:hadoop hadoop

nano $HOME/.bashrc
###now paste this code in ubuntu shell in end -> 

#Hadoop Related Options
export HADOOP_HOME=/usr/local/hadoop
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

###press Ctrl+X, Shift Y and Enter

source ~/.bashrc

cd /usr/local/hadoop/etc/hadoop

nano hadoop-env.sh
###now paste this code in ubuntu shell in end -> 

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64

###press Ctrl+X, Shift Y and Enter

sudo mkdir -p /app/hadoop/tmp

sudo chown universal_user:hadoop /app/hadoop/tmp

nano core-site.xml

###now paste this code in ubuntu shell in end -> 
###Add the following between <configuration> and </configuration>

<property>
<name>hadoop.tmp.dir</name>
<value>/app/hadoop/tmp</value>
</property>
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:54310</value>
</property>

###press Ctrl+X, Shift Y and Enter

nano mapred-site.sh
###now paste this code in ubuntu shell in end -> 

<configuration>
<property>
<name>mapred.job.tracker</name>
<value>localhost:54311</value>
</property>
</configuration>

###press Ctrl+X, Shift Y and Enter

nano hdfs-site.xml
###now paste this code in ubuntu shell in end -> 

<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
</configuration>

###press Ctrl+X, Shift Y and Enter

hadoop namenode -format

ssh

sudo apt-get install ssh

ssh localhost

cd /usr/local/hadoop/sbin

start-all.sh

jps

stop-all.sh


########## P2 ##########


###Open cmd and start firing commands

> wsl
> mkdir MapReduce
> cd MapReduce
> touch word_count_data.txt
> nano word_count_data.txt

###now paste this code in ubuntu shell -> 

Hello wordcount MapReduce Hadoop program.
This is my first MapReduce program.

###press Ctrl+X, Shift Y and Enter

> cat word_count_data.txt
> touch mapper.py
> nano mapper.py

###now paste this code in ubuntu shell -> 

import sys
for line in sys.stdin:
 line = line.strip()
 words = line.split()
 for word in words:
  print('%s\t%s' % (word, 1))

###press Ctrl+X, Shift Y and Enter

> cat word_count_data.txt | python3 mapper.py
> touch reducer.py
> nano reducer.py

###now paste this code in ubuntu shell -> 

from operator import itemgetter
import sys
current_word = None
current_count = 0
word = None
for line in sys.stdin:
 line = line.strip()
 word, count = line.split('\t', 1)
 try:
   count = int(count)
 except ValueError:
   continue
 if current_word == word:
   current_count += count
 else:
   if current_word:
     print('%s\t%s' % (current_word, current_count))
   current_count = count
   current_word = word
if current_word == word:
 print('%s\t%s' % (current_word, current_count))

###press Ctrl+X, Shift Y and Enter

> cat word_count_data.txt | python3 mapper.py | sort -k1,1 | python3 reducer.py




########## P3 ##########


###Open cmd and start firing commands

> wsl
> cd /home/universal_user
> sudo nano employees.txt

###Add following details in the employees.txt ->

1-Mark-US-Sales-1800
2-Ram-India-Developer-2000
3-shyam-India-Engineer-2500
4-Ann-UK-Recruiter-1500

###Save “Ctrl S” and exit the file by pressing “Ctrl X”

> cd /usr/local
> sudo wget https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
> sudo tar -xvzf apache-hive-3.1.2-bin.tar.gz
> sudo mv apache-hive-3.1.2-bin hive
> ls -l
> sudo chmod 777 hive
> ls -l
> cd /home/universal_user
> nano .bashrc

###now paste this code in ubuntu shell -> 

#Hive Related Options
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin

###press Ctrl+X, Shift Y and Enter

> source .bashrc
> cd /usr/local/hive/bin
> sudo nano hive-config.sh

###now paste this code in ubuntu shell -> 

export HADOOP_HOME=/usr/local/hadoop

###press Ctrl+X, Shift Y and Enter

> cd /home/universal_user
> start-all.sh
> hdfs dfs -mkdir /tmp
> hdfs dfs -chmod g+w /tmp
> hdfs dfs -ls /
> hdfs dfs -mkdir -p /user/hive/warehouse
> hdfs dfs -chmod g+w /user/hive/warehouse
> hdfs dfs -ls /user/hive
> ls
> cd /usr/local/hive/bin
> cd $HIVE_HOME/conf
> nano hive-env.sh

###now paste this code in ubuntu shell -> 

HADOOP_HOME=/usr/local/hadoop

###press Ctrl+X, Shift Y and Enter

> schematool -initSchema -dbType derby
> cd $HIVE_HOME
> hive

###now paste the commands in database shell -> 

1. show databases;
2. create database company;
3. show databases;
4. use company;
5. create table employees (id int, name string, country string, department string, salary int)
6.show tables;
7. load data inpath '/home/hadoop/hive/employees.txt' overwrite into table employees;
8. select * from employees;
9. quit



########## P4 ##########


###Open cmd and start firing commands

> wsl
> cd /usr/local
> sudo wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
> sudo tar -xvzf spark-3.2.1-bin-hadoop3.2.tgz
> sudo mv spark-3.2.1-bin-hadoop3.2 spark
> sudo chmod 777 spark
> sudo nano ~/.bashrc

###now paste this code in ubuntu shell ->

export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin

###press Ctrl+X, Shift Y and Enter

> source ~/.bashrc
> spark-shell

> :quit


########## P5 ##########


###Open cmd and start firing commands

> wsl
> su universal_user
> cd /usr/local
> sudo wget https://archive.apache.org/dist/hbase/2.4.2/hbase-2.4.2-bin.tar.gz
> sudo tar xzvf hbase-2.4.2-bin.tar.gz
> sudo mv hbase-2.4.2 hbase
> cd hbase/conf
> sudo nano hbase-env.sh

###now paste this code in ubuntu shell ->

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64

###press Ctrl+X, Shift Y and Enter

> sudo nano hbase-site.xml

###now paste this code in ubuntu shell ->

<property>
<name>hbase.rootdir</name>
<value>file:///usr/local/hbase</value>
</property>
<property>
<name>hbase.zookeeper.property.dataDir</name>
<value>/usr/local/hbase/zookeeper</value>
</property>

###press Ctrl+X, Shift Y and Enter

> cd /usr/local
> sudo chmod 777 hbase
> cd /usr/local/hbase/bin
> ./start-hbase.sh
> ./hbase shell

###Execute below commands on HBase ->

1. create 'test', 'cf'
2. put 'test', 'row1', 'cf:a', 'Bob'
3. put 'test', 'row2', 'cf:b', '01'
4. put 'test', 'row3', 'cf:c', 'Mumbai'
5. scan 'test'
6. exit


> ./stop-hbase.sh
> cd /home/universal_user
> sudo apt install python3-pip
> pip3 install happybase
> cd /usr/local/hbase/bin
> ./hbase-daemon.sh start thrift
> ./start-hbase.sh
> python3

###Enter Python Syntax

>>> import happybase as hb
>>> conn=hb.Connection('127.0.0.1', 9090)
>>> conn.table('test').row('row1')
>>> conn.table('test').row('row2')
>>> conn.table('test').row('row3')
>>> exit()


> ./stop-hbase.sh
> ./hbase-daemon.sh stop thrift
> sudo service ssh start
> ssh localhost
> /usr/local/hadoop/sbin/start-all.sh
> jps
> cd /usr/local/hbase/bin
> ./start-hbase.sh
> ./hbase shell

###Execute below commands on HBase ->

1. scan 'test'
2. exit



> cd /usr/local
> sudo nano simple1.txt

###now paste this code in ubuntu shell ->

1,Patkar
2,Mithibai
3,VIVA

###press Ctrl+X, Shift Y and Enter

> hdfs dfs -copyFromLocal /usr/local/simple1.txt /
> cd /usr/local/hbase/bin
> ./hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator="," -Dimporttsv.columns=HBASE_ROW_KEY,cf test /simple1.txt
> ./hbase shell

###Execute below commands on HBase ->

1. scan 'test'
2. exit

###press Ctrl+X, Shift Y and Enter

> cd /usr/local/hbase/bin
> ./hbase org.apache.hadoop.hbase.mapreduce.Export test /location
> hdfs dfs -ls /location
> ./stop-hbase.sh
> stop-all.sh


########## P6 ##########


#Use this to remove any pre installation of MySQL Server

sudo systemctl stop mysql
sudo apt-get purge mysql-server mysql-client mysql-common mysql-server-core-* mysql-client-core-*
sudo rm -rf /etc/mysql /var/lib/mysql
sudo apt autoremove
sudo apt autoclean

sudo apt update && sudo apt upgrade -y

sudo apt install mysql-server

sudo service mysql start

sudo mysql -u root -p

alter user 'root'@'localhost' identified with mysql_native_password by 'a';
alter user 'root'@'localhost' identified by 'a';
flush privileges;
quit;

sudo mysql_secure_installation
#setup Validate password component Press yIY for Yes, any other key for No: y 
#Please enter 0 = LOW, 1 = MEDIUM and 2 = STRONG: 0
#Change the password for root? ((Press y|Y for Yes, any other key for No): n 
#Remove anonymous users? (Press y|Y for Yes, any other key for No): y 
#Disallow root login remotely? (Press ylY for Yes, any other key for No): y 
#Remove the test database and access to it? (Press yIY for Yes, any other key for No): n 
#Reload privilege tables now? (Press y|Y for Yes, any other key for No): y


wget https://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop2.6.0.tar.gz

tar xvzf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz

nano ~/.bashrc
#Add these lines
export SQOOP_HOME=/home/hadoop/sqoop-1.4.7.bin__hadoop-2.6.0
export PATH:$SQOOP_HOME/bin
export CLASSPATH:$CLASSPATH:$SQOOP_HOME/lib/*

cd $SQOOP_HOME/

nano sqoop-env.sh
#Add this line
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME

chmod ugo+x sqoop-env.sh

wget https://dev.mysql.com/get/Downloads/ConnectorJ/mysql-connector-java-8.0.28.tar.gz

tar xvzf mysql-connector-java-8.0.28.tar.gz

mv mysql-connector-java-8.0.28/ mysql-connectorjava-8.0.28.jar /home/Hadoop/downloads/sqoop-1.4.7.bin__hadoop-2.6.0/lib/

wget https://dlcdn.apache.org//commons/lang/binaries/commons-lang-2.6-bin.tar.gz

tar xzf commons-lang-2.6-bin.tar.gz

mv commonslang-2.6.jar /home/hadoop/sqoop-1.4.7.bin__hadoop-2.6.0/lib

sudo service mysql restart

sudo mysql -u root -p
#On prompting password enter a

show databases;
create database hr;
use hr;
create table student(id int primary key);
insert into student values(10);
insert into student values(20);
select * from student;
quit

sudo service ssh restart
ssh localhost
start-all.sh
jps

hdfs dfs -rm -r hdfs://localhost:54310/user/[username]/student

cd sqoop-1.4.7.bin__hadoop-2.6.0/

sqoop listdatabases --connect jdbc:mysql://localhost --username root --password a

sqoop list-tables --connect jdbc:mysql://localhost/hr --username root --password a

sqoop import --connect jdbc:mysql://localhost/hr --username root --password a --table student --m 1 --bindir .

stop-all.sh


########## P7 ##########


nano sample.txt
#Add these lines
1, Sanvi, 20000, Lecturer
2, Manoj, 25000, Accountant
3, Rahul, 30000, IT

cat sample.txt

wget https://downloads.apache.org/pig/latest/pig-0.17.0.tar.gz

tar -xvzf pig-0.17.0.tar.gz pig-0.17.0/

chmod 777 pig-0.17.0

mv pig-0.17.0 /usr/local/

nano ~/.bashrc

source ~/.bashrc

pig
quit

pig -x local
Employees = LOAD 'sample.txt' USING PigStorage(',') as (id:int, firstname:chararray, salary:int, dept:chararray);
Dump Employees;
Employees_Order = ORDER Employees BY salary DESC;
Dump Employees_Order;
quit


########## P8 ##########


###Open cmd and start firing commands

> wsl
> cd /usr/local
> sudo wget https://archive.apache.org/dist/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz
> sudo wget https://archive.apache.org/dist/flume/1.6.0/apache-flume-1.6.0-src.tar.gz
> sudo tar zxvf apache-flume-1.6.0-bin.tar.gz
> nano ~/.bashrc

###now paste this code in ubuntu shell ->

export FLUME_HOME=/usr/local/apache-flume-1.6.0-bin
export PATH=$PATH:$FLUME_HOME/bin

###press Ctrl+X, Shift Y and Enter

> source ~/.bashrc
> flume-ng version
> cd apache-flume-1.6.0-bin/conf/
> sudo touch flume.conf
> sudo nano flume.conf

###now paste this code in ubuntu shell in the end->

agent1.sources = tail
agent1.channels = Channel-2
agent1.sinks = sink-1
agent1.sources.tail.type = exec
agent1.sources.tail.command = cat /home/monika/data
agent1.sources.tail.channels = Channel-2
agent1.sinks.sink-1.channel = Channel-2
agent1.sinks.sink-1.type = hdfs
agent1.sinks.sink-1.hdfs.path = hdfs://localhost:9000/flume01
agent1.sinks.sink-1.hdfs.fileType = DataStream
agent1.sinks.sink-1.hdfs.rollInterval = 60
agent1.sinks.sink-1.hdfs.rollSize = 0
agent1.sinks.sink-1.hdfs.rollCount = 0
agent1.channels.Channel-2.type = memory


###press Ctrl+X, Shift Y and Enter

> cd /usr/local
> cd apache-flume-1.6.0-bin
> bin/flume-ng agent –conf ./conf/ -f conf/flume.conf -n agent1 -Dflume.root.logger=DEBUG


